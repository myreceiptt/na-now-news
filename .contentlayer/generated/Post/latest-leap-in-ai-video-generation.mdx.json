{
  "title": "The Latest Developments in AI, Especially Its Progress in The Realm of Art.",
  "date": "2024-02-29T00:00:00.000Z",
  "description": "Artificial intelligence continues to transform content creation, and OpenAI's Sora is leading the charge in video generation. Announced in early 2024, Sora represents a significant breakthrough in text-to-video (TTV) models, offering creators the ability to generate realistic videos from simple text prompts.",
  "categories": [
    "Arts",
    "Reviews",
    "Technology"
  ],
  "gambar": "/images/posts/latest-leap-in-ai-video-generation-0.webp",
  "penulis": "Prof. NOTA",
  "link": "https://nota.endhonesa.com/",
  "body": {
    "raw": "\n---\n\n# Hi, OiOi Fams!!!!\n\nEven though we are on a rich island, namely [**BANANOW**._LAND_](https://www.bananow.land/ \"BANANOW.LAND Official Website\"), we don't want to miss out on the latest developments in **AI**, especially its progress in the realm of art.\n\n_Artificial_ **intelligence** continues to transform content creation, and [_OpenAI's Sora_](https://openai.com/index/sora/) is leading the charge in video generation. Announced in early 2024, **Sora** represents a significant breakthrough in text-to-video (TTV) models, offering creators the ability to generate realistic videos from simple text prompts. As we anticipate its full public release later this year, let's explore what _Sora_ brings to the table and how it could reshape the future of **AI**-driven content creation.\n\n##### What is **Sora AI**?\n\n_Sora_ is OpenAI’s state-of-the-art video generation tool, designed to create short, high-fidelity videos of up to 60 seconds. Users simply input a detailed text prompt, and **Sora** generates videos that can include complex scenes, simulated camera movements, and realistic environmental interactions. From smooth transitions to coherent character designs, _Sora_ offers capabilities that surpass previous **AI** models, such as Google’s Lumiere​ (Source: DailyAI).\n\nThis innovation is rooted in OpenAI’s deep understanding of how objects interact in the physical world. _Sora_ demonstrates impressive physics-based realism, evident in videos that feature dynamic lighting, realistic shadows, and environmental effects like footprints in the sand​ (Source: DailyAI).\n\n<YouTube id=\"HK6y8DAPN_0\" />\n\n---\n\n##### Key Features of **Sora**\n\n1. Text-to-Video Generation: _Sora_ transforms text-based descriptions into visually compelling videos. Prompts like “A movie trailer featuring a 30-year-old spaceman in a red helmet walking across a salt desert” produce highly detailed and engaging video clips​ (Source: DailyAI).\n\n2. Realism and Scene Coherence: **Sora** ensures that generated objects and characters remain consistent throughout the video, even when they leave and reappear within the frame. The model also preserves environmental details across scenes, such as reflections and weather conditions​ (Source: Neowin).\n\n3. Emerging Abilities: Without being explicitly trained in 3D physics, _Sora_ demonstrates emergent capabilities, such as understanding how objects behave under natural forces (lighting, shadows, etc.). This positions **Sora** as a foundation for future models that could simulate even more complex interactions​ (Source: DailyAI).\n\n---\n\n##### Limitations and Upcoming Features\n\nCurrently, _Sora_-generated videos are silent, and there is no in-process editing during video creation. However, OpenAI is actively working to incorporate audio and editing features into the platform​ (Source: Neowin). These improvements, once rolled out, will likely elevate the tool's capabilities even further, making it more useful for video creators, filmmakers, and marketers.\n\nOpenAI also revealed that **Sora** relies on licensed content from partners like Shutterstock to train its model, though the full list of data sources remains undisclosed​ (Source: Neowin).\n\n---\n\n##### Public Release and Pricing\n\n_Sora_ has been in a limited testing phase, accessible to select visual artists, designers, and \"red teamers\" to ensure the model's safety and reliability. However, OpenAI plans to release **Sora** to the public by the end of 2024​ (Source: Neowin). Pricing details are yet to be finalized, but it is expected to follow a model similar to OpenAI's DALL-E 3, though _Sora_’s computational power requirements are significantly higher​ (Source: Neowin).\n\n---\n\n##### How **Sora** Compares to Competitors\n\n_Sora_ isn’t the only player in the text-to-video space. Competitors like Google's Lumiere have made strides in generating 5-second video clips, but **Sora**'s ability to create longer, more complex videos with realistic physics sets it apart​ (Source: DailyAI).\n\n---\n\n##### What Does _Sora_ Mean for Content Creators?\n\nFor bloggers, marketers, filmmakers, and anyone in the creative space, **Sora** represents a new frontier in video production. Its ability to generate quick, compelling content from textual descriptions could drastically reduce the time and resources needed to produce high-quality videos. As AI-generated content continues to grow in sophistication, tools like _Sora_ could democratize video production, empowering smaller creators to compete with larger media companies.\n\n---\n\n##### Conclusion\n\nOpenAI’s **Sora** is poised to revolutionize how we think about video creation, making it easier for creators to bring their ideas to life. With its ability to produce realistic, dynamic videos from text, _Sora_ has the potential to reshape industries ranging from filmmaking to digital marketing. Stay tuned as this groundbreaking **AI** technology rolls out publicly later in 2024.\n\nFor updates and further details, follow OpenAI’s official channels and stay ahead of the latest in _AI_ innovations.\n\n**#ALLHAILAI**\\\n_#PromptArtist_\n",
    "code": "var Component=(()=>{var mn=Object.create;var D=Object.defineProperty;var bn=Object.getOwnPropertyDescriptor;var pn=Object.getOwnPropertyNames;var _n=Object.getPrototypeOf,hn=Object.prototype.hasOwnProperty;var q=(f,n)=>()=>(n||f((n={exports:{}}).exports,n),n.exports),wn=(f,n)=>{for(var m in n)D(f,m,{get:n[m],enumerable:!0})},xe=(f,n,m,v)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let N of pn(n))!hn.call(f,N)&&N!==m&&D(f,N,{get:()=>n[N],enumerable:!(v=bn(n,N))||v.enumerable});return f};var Nn=(f,n,m)=>(m=f!=null?mn(_n(f)):{},xe(n||!f||!f.__esModule?D(m,\"default\",{value:f,enumerable:!0}):m,f)),vn=f=>xe(D({},\"__esModule\",{value:!0}),f);var Se=q((Rn,Ee)=>{Ee.exports=React});var Ue=q(z=>{\"use strict\";(function(){\"use strict\";var f=Se(),n=Symbol.for(\"react.element\"),m=Symbol.for(\"react.portal\"),v=Symbol.for(\"react.fragment\"),N=Symbol.for(\"react.strict_mode\"),X=Symbol.for(\"react.profiler\"),J=Symbol.for(\"react.provider\"),Z=Symbol.for(\"react.context\"),A=Symbol.for(\"react.forward_ref\"),I=Symbol.for(\"react.suspense\"),F=Symbol.for(\"react.suspense_list\"),O=Symbol.for(\"react.memo\"),W=Symbol.for(\"react.lazy\"),Oe=Symbol.for(\"react.offscreen\"),Q=Symbol.iterator,Ce=\"@@iterator\";function ke(e){if(e===null||typeof e!=\"object\")return null;var r=Q&&e[Q]||e[Ce];return typeof r==\"function\"?r:null}var x=f.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function b(e){{for(var r=arguments.length,t=new Array(r>1?r-1:0),i=1;i<r;i++)t[i-1]=arguments[i];Pe(\"error\",e,t)}}function Pe(e,r,t){{var i=x.ReactDebugCurrentFrame,l=i.getStackAddendum();l!==\"\"&&(r+=\"%s\",t=t.concat([l]));var u=t.map(function(s){return String(s)});u.unshift(\"Warning: \"+r),Function.prototype.apply.call(console[e],console,u)}}var je=!1,De=!1,Ie=!1,Fe=!1,We=!1,ee;ee=Symbol.for(\"react.module.reference\");function Le(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===v||e===X||We||e===N||e===I||e===F||Fe||e===Oe||je||De||Ie||typeof e==\"object\"&&e!==null&&(e.$$typeof===W||e.$$typeof===O||e.$$typeof===J||e.$$typeof===Z||e.$$typeof===A||e.$$typeof===ee||e.getModuleId!==void 0))}function Ye(e,r,t){var i=e.displayName;if(i)return i;var l=r.displayName||r.name||\"\";return l!==\"\"?t+\"(\"+l+\")\":t}function ne(e){return e.displayName||\"Context\"}function w(e){if(e==null)return null;if(typeof e.tag==\"number\"&&b(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case v:return\"Fragment\";case m:return\"Portal\";case X:return\"Profiler\";case N:return\"StrictMode\";case I:return\"Suspense\";case F:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case Z:var r=e;return ne(r)+\".Consumer\";case J:var t=e;return ne(t._context)+\".Provider\";case A:return Ye(e,e.render,\"ForwardRef\");case O:var i=e.displayName||null;return i!==null?i:w(e.type)||\"Memo\";case W:{var l=e,u=l._payload,s=l._init;try{return w(s(u))}catch{return null}}}return null}var y=Object.assign,U=0,re,ae,te,ie,oe,se,le;function fe(){}fe.__reactDisabledLog=!0;function Ve(){{if(U===0){re=console.log,ae=console.info,te=console.warn,ie=console.error,oe=console.group,se=console.groupCollapsed,le=console.groupEnd;var e={configurable:!0,enumerable:!0,value:fe,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}U++}}function $e(){{if(U--,U===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:y({},e,{value:re}),info:y({},e,{value:ae}),warn:y({},e,{value:te}),error:y({},e,{value:ie}),group:y({},e,{value:oe}),groupCollapsed:y({},e,{value:se}),groupEnd:y({},e,{value:le})})}U<0&&b(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var L=x.ReactCurrentDispatcher,Y;function C(e,r,t){{if(Y===void 0)try{throw Error()}catch(l){var i=l.stack.trim().match(/\\n( *(at )?)/);Y=i&&i[1]||\"\"}return`\n`+Y+e}}var V=!1,k;{var Me=typeof WeakMap==\"function\"?WeakMap:Map;k=new Me}function ue(e,r){if(!e||V)return\"\";{var t=k.get(e);if(t!==void 0)return t}var i;V=!0;var l=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var u;u=L.current,L.current=null,Ve();try{if(r){var s=function(){throw Error()};if(Object.defineProperty(s.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(s,[])}catch(_){i=_}Reflect.construct(e,[],s)}else{try{s.call()}catch(_){i=_}e.call(s.prototype)}}else{try{throw Error()}catch(_){i=_}e()}}catch(_){if(_&&i&&typeof _.stack==\"string\"){for(var o=_.stack.split(`\n`),p=i.stack.split(`\n`),d=o.length-1,c=p.length-1;d>=1&&c>=0&&o[d]!==p[c];)c--;for(;d>=1&&c>=0;d--,c--)if(o[d]!==p[c]){if(d!==1||c!==1)do if(d--,c--,c<0||o[d]!==p[c]){var h=`\n`+o[d].replace(\" at new \",\" at \");return e.displayName&&h.includes(\"<anonymous>\")&&(h=h.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&k.set(e,h),h}while(d>=1&&c>=0);break}}}finally{V=!1,L.current=u,$e(),Error.prepareStackTrace=l}var S=e?e.displayName||e.name:\"\",g=S?C(S):\"\";return typeof e==\"function\"&&k.set(e,g),g}function Be(e,r,t){return ue(e,!1)}function Ke(e){var r=e.prototype;return!!(r&&r.isReactComponent)}function P(e,r,t){if(e==null)return\"\";if(typeof e==\"function\")return ue(e,Ke(e));if(typeof e==\"string\")return C(e);switch(e){case I:return C(\"Suspense\");case F:return C(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case A:return Be(e.render);case O:return P(e.type,r,t);case W:{var i=e,l=i._payload,u=i._init;try{return P(u(l),r,t)}catch{}}}return\"\"}var R=Object.prototype.hasOwnProperty,de={},ce=x.ReactDebugCurrentFrame;function j(e){if(e){var r=e._owner,t=P(e.type,e._source,r?r.type:null);ce.setExtraStackFrame(t)}else ce.setExtraStackFrame(null)}function Ge(e,r,t,i,l){{var u=Function.call.bind(R);for(var s in e)if(u(e,s)){var o=void 0;try{if(typeof e[s]!=\"function\"){var p=Error((i||\"React class\")+\": \"+t+\" type `\"+s+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[s]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw p.name=\"Invariant Violation\",p}o=e[s](r,s,i,t,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(d){o=d}o&&!(o instanceof Error)&&(j(l),b(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",i||\"React class\",t,s,typeof o),j(null)),o instanceof Error&&!(o.message in de)&&(de[o.message]=!0,j(l),b(\"Failed %s type: %s\",t,o.message),j(null))}}}var He=Array.isArray;function $(e){return He(e)}function qe(e){{var r=typeof Symbol==\"function\"&&Symbol.toStringTag,t=r&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return t}}function ze(e){try{return me(e),!1}catch{return!0}}function me(e){return\"\"+e}function be(e){if(ze(e))return b(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",qe(e)),me(e)}var T=x.ReactCurrentOwner,Xe={key:!0,ref:!0,__self:!0,__source:!0},pe,_e,M;M={};function Je(e){if(R.call(e,\"ref\")){var r=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(r&&r.isReactWarning)return!1}return e.ref!==void 0}function Ze(e){if(R.call(e,\"key\")){var r=Object.getOwnPropertyDescriptor(e,\"key\").get;if(r&&r.isReactWarning)return!1}return e.key!==void 0}function Qe(e,r){if(typeof e.ref==\"string\"&&T.current&&r&&T.current.stateNode!==r){var t=w(T.current.type);M[t]||(b('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',w(T.current.type),e.ref),M[t]=!0)}}function en(e,r){{var t=function(){pe||(pe=!0,b(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",r))};t.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:t,configurable:!0})}}function nn(e,r){{var t=function(){_e||(_e=!0,b(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",r))};t.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:t,configurable:!0})}}var rn=function(e,r,t,i,l,u,s){var o={$$typeof:n,type:e,key:r,ref:t,props:s,_owner:u};return o._store={},Object.defineProperty(o._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(o,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:i}),Object.defineProperty(o,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:l}),Object.freeze&&(Object.freeze(o.props),Object.freeze(o)),o};function an(e,r,t,i,l){{var u,s={},o=null,p=null;t!==void 0&&(be(t),o=\"\"+t),Ze(r)&&(be(r.key),o=\"\"+r.key),Je(r)&&(p=r.ref,Qe(r,l));for(u in r)R.call(r,u)&&!Xe.hasOwnProperty(u)&&(s[u]=r[u]);if(e&&e.defaultProps){var d=e.defaultProps;for(u in d)s[u]===void 0&&(s[u]=d[u])}if(o||p){var c=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;o&&en(s,c),p&&nn(s,c)}return rn(e,o,p,l,i,T.current,s)}}var B=x.ReactCurrentOwner,he=x.ReactDebugCurrentFrame;function E(e){if(e){var r=e._owner,t=P(e.type,e._source,r?r.type:null);he.setExtraStackFrame(t)}else he.setExtraStackFrame(null)}var K;K=!1;function G(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===n}function we(){{if(B.current){var e=w(B.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function tn(e){{if(e!==void 0){var r=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),t=e.lineNumber;return`\n\nCheck your code at `+r+\":\"+t+\".\"}return\"\"}}var Ne={};function on(e){{var r=we();if(!r){var t=typeof e==\"string\"?e:e.displayName||e.name;t&&(r=`\n\nCheck the top-level render call using <`+t+\">.\")}return r}}function ve(e,r){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var t=on(r);if(Ne[t])return;Ne[t]=!0;var i=\"\";e&&e._owner&&e._owner!==B.current&&(i=\" It was passed a child from \"+w(e._owner.type)+\".\"),E(e),b('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',t,i),E(null)}}function ye(e,r){{if(typeof e!=\"object\")return;if($(e))for(var t=0;t<e.length;t++){var i=e[t];G(i)&&ve(i,r)}else if(G(e))e._store&&(e._store.validated=!0);else if(e){var l=ke(e);if(typeof l==\"function\"&&l!==e.entries)for(var u=l.call(e),s;!(s=u.next()).done;)G(s.value)&&ve(s.value,r)}}}function sn(e){{var r=e.type;if(r==null||typeof r==\"string\")return;var t;if(typeof r==\"function\")t=r.propTypes;else if(typeof r==\"object\"&&(r.$$typeof===A||r.$$typeof===O))t=r.propTypes;else return;if(t){var i=w(r);Ge(t,e.props,\"prop\",i,e)}else if(r.PropTypes!==void 0&&!K){K=!0;var l=w(r);b(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",l||\"Unknown\")}typeof r.getDefaultProps==\"function\"&&!r.getDefaultProps.isReactClassApproved&&b(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function ln(e){{for(var r=Object.keys(e.props),t=0;t<r.length;t++){var i=r[t];if(i!==\"children\"&&i!==\"key\"){E(e),b(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",i),E(null);break}}e.ref!==null&&(E(e),b(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),E(null))}}var ge={};function fn(e,r,t,i,l,u){{var s=Le(e);if(!s){var o=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(o+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var p=tn(l);p?o+=p:o+=we();var d;e===null?d=\"null\":$(e)?d=\"array\":e!==void 0&&e.$$typeof===n?(d=\"<\"+(w(e.type)||\"Unknown\")+\" />\",o=\" Did you accidentally export a JSX literal instead of a component?\"):d=typeof e,b(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",d,o)}var c=an(e,r,t,l,u);if(c==null)return c;if(s){var h=r.children;if(h!==void 0)if(i)if($(h)){for(var S=0;S<h.length;S++)ye(h[S],e);Object.freeze&&Object.freeze(h)}else b(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else ye(h,e)}if(R.call(r,\"key\")){var g=w(e),_=Object.keys(r).filter(function(cn){return cn!==\"key\"}),H=_.length>0?\"{key: someKey, \"+_.join(\": ..., \")+\": ...}\":\"{key: someKey}\";if(!ge[g+H]){var dn=_.length>0?\"{\"+_.join(\": ..., \")+\": ...}\":\"{}\";b(`A props object containing a \"key\" prop is being spread into JSX:\n  let props = %s;\n  <%s {...props} />\nReact keys must be passed directly to JSX without using spread:\n  let props = %s;\n  <%s key={someKey} {...props} />`,H,g,dn,g),ge[g+H]=!0}}return e===v?ln(c):sn(c),c}}var un=fn;z.Fragment=v,z.jsxDEV=un})()});var Te=q((An,Re)=>{\"use strict\";Re.exports=Ue()});var Sn={};wn(Sn,{default:()=>xn,frontmatter:()=>yn});var a=Nn(Te()),yn={title:\"The Latest Developments in AI, Especially Its Progress in The Realm of Art.\",date:new Date(17091648e5),description:\"Artificial intelligence continues to transform content creation, and OpenAI's Sora is leading the charge in video generation. Announced in early 2024, Sora represents a significant breakthrough in text-to-video (TTV) models, offering creators the ability to generate realistic videos from simple text prompts.\",categories:[\"Arts\",\"Reviews\",\"Technology\"],gambar:\"/images/posts/latest-leap-in-ai-video-generation-0.webp\",penulis:\"Prof. NOTA\",link:\"https://nota.endhonesa.com/\"};function Ae(f){let n=Object.assign({hr:\"hr\",h1:\"h1\",p:\"p\",a:\"a\",strong:\"strong\",em:\"em\",h5:\"h5\",ol:\"ol\",li:\"li\",br:\"br\"},f.components),{YouTube:m}=n;return m||En(\"YouTube\",!0,\"25:1-25:29\"),(0,a.jsxDEV)(a.Fragment,{children:[(0,a.jsxDEV)(n.hr,{},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:11,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.h1,{children:\"Hi, OiOi Fams!!!!\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:13,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[\"Even though we are on a rich island, namely \",(0,a.jsxDEV)(n.a,{href:\"https://www.bananow.land/\",title:\"BANANOW.LAND Official Website\",children:[(0,a.jsxDEV)(n.strong,{children:\"BANANOW\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:15,columnNumber:46},this),\".\",(0,a.jsxDEV)(n.em,{children:\"LAND\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:15,columnNumber:58},this)]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:15,columnNumber:45},this),\", we don't want to miss out on the latest developments in \",(0,a.jsxDEV)(n.strong,{children:\"AI\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:15,columnNumber:182},this),\", especially its progress in the realm of art.\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:15,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[(0,a.jsxDEV)(n.em,{children:\"Artificial\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:17,columnNumber:1},this),\" \",(0,a.jsxDEV)(n.strong,{children:\"intelligence\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:17,columnNumber:14},this),\" continues to transform content creation, and \",(0,a.jsxDEV)(n.a,{href:\"https://openai.com/index/sora/\",children:(0,a.jsxDEV)(n.em,{children:\"OpenAI's Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:17,columnNumber:77},this)},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:17,columnNumber:76},this),\" is leading the charge in video generation. Announced in early 2024, \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:17,columnNumber:194},this),\" represents a significant breakthrough in text-to-video (TTV) models, offering creators the ability to generate realistic videos from simple text prompts. As we anticipate its full public release later this year, let's explore what \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:17,columnNumber:434},this),\" brings to the table and how it could reshape the future of \",(0,a.jsxDEV)(n.strong,{children:\"AI\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:17,columnNumber:500},this),\"-driven content creation.\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:17,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.h5,{children:[\"What is \",(0,a.jsxDEV)(n.strong,{children:\"Sora AI\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:19,columnNumber:15},this),\"?\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:19,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:21,columnNumber:1},this),\" is OpenAI\\u2019s state-of-the-art video generation tool, designed to create short, high-fidelity videos of up to 60 seconds. Users simply input a detailed text prompt, and \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:21,columnNumber:175},this),\" generates videos that can include complex scenes, simulated camera movements, and realistic environmental interactions. From smooth transitions to coherent character designs, \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:21,columnNumber:359},this),\" offers capabilities that surpass previous \",(0,a.jsxDEV)(n.strong,{children:\"AI\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:21,columnNumber:408},this),\" models, such as Google\\u2019s Lumiere\\u200B (Source: DailyAI).\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:21,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[\"This innovation is rooted in OpenAI\\u2019s deep understanding of how objects interact in the physical world. \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:23,columnNumber:105},this),\" demonstrates impressive physics-based realism, evident in videos that feature dynamic lighting, realistic shadows, and environmental effects like footprints in the sand\\u200B (Source: DailyAI).\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:23,columnNumber:1},this),`\n`,(0,a.jsxDEV)(m,{id:\"HK6y8DAPN_0\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:25,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.hr,{},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:27,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.h5,{children:[\"Key Features of \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:29,columnNumber:23},this)]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:29,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.ol,{children:[`\n`,(0,a.jsxDEV)(n.li,{children:[`\n`,(0,a.jsxDEV)(n.p,{children:[\"Text-to-Video Generation: \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:31,columnNumber:30},this),\" transforms text-based descriptions into visually compelling videos. Prompts like \\u201CA movie trailer featuring a 30-year-old spaceman in a red helmet walking across a salt desert\\u201D produce highly detailed and engaging video clips\\u200B (Source: DailyAI).\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:31,columnNumber:4},this),`\n`]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:31,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.li,{children:[`\n`,(0,a.jsxDEV)(n.p,{children:[\"Realism and Scene Coherence: \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:33,columnNumber:33},this),\" ensures that generated objects and characters remain consistent throughout the video, even when they leave and reappear within the frame. The model also preserves environmental details across scenes, such as reflections and weather conditions\\u200B (Source: Neowin).\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:33,columnNumber:4},this),`\n`]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:33,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.li,{children:[`\n`,(0,a.jsxDEV)(n.p,{children:[\"Emerging Abilities: Without being explicitly trained in 3D physics, \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:35,columnNumber:72},this),\" demonstrates emergent capabilities, such as understanding how objects behave under natural forces (lighting, shadows, etc.). This positions \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:35,columnNumber:219},this),\" as a foundation for future models that could simulate even more complex interactions\\u200B (Source: DailyAI).\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:35,columnNumber:4},this),`\n`]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:35,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:31,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.hr,{},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:37,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.h5,{children:\"Limitations and Upcoming Features\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:39,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[\"Currently, \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:41,columnNumber:12},this),\"-generated videos are silent, and there is no in-process editing during video creation. However, OpenAI is actively working to incorporate audio and editing features into the platform\\u200B (Source: Neowin). These improvements, once rolled out, will likely elevate the tool's capabilities even further, making it more useful for video creators, filmmakers, and marketers.\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:41,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[\"OpenAI also revealed that \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:43,columnNumber:27},this),\" relies on licensed content from partners like Shutterstock to train its model, though the full list of data sources remains undisclosed\\u200B (Source: Neowin).\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:43,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.hr,{},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:45,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.h5,{children:\"Public Release and Pricing\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:47,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:49,columnNumber:1},this),` has been in a limited testing phase, accessible to select visual artists, designers, and \"red teamers\" to ensure the model's safety and reliability. However, OpenAI plans to release `,(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:49,columnNumber:190},this),\" to the public by the end of 2024\\u200B (Source: Neowin). Pricing details are yet to be finalized, but it is expected to follow a model similar to OpenAI's DALL-E 3, though \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:49,columnNumber:366},this),\"\\u2019s computational power requirements are significantly higher\\u200B (Source: Neowin).\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:49,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.hr,{},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:51,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.h5,{children:[\"How \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:53,columnNumber:11},this),\" Compares to Competitors\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:53,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:55,columnNumber:1},this),\" isn\\u2019t the only player in the text-to-video space. Competitors like Google's Lumiere have made strides in generating 5-second video clips, but \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:55,columnNumber:150},this),\"'s ability to create longer, more complex videos with realistic physics sets it apart\\u200B (Source: DailyAI).\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:55,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.hr,{},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:57,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.h5,{children:[\"What Does \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:59,columnNumber:17},this),\" Mean for Content Creators?\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:59,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[\"For bloggers, marketers, filmmakers, and anyone in the creative space, \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:61,columnNumber:72},this),\" represents a new frontier in video production. Its ability to generate quick, compelling content from textual descriptions could drastically reduce the time and resources needed to produce high-quality videos. As AI-generated content continues to grow in sophistication, tools like \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:61,columnNumber:363},this),\" could democratize video production, empowering smaller creators to compete with larger media companies.\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:61,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.hr,{},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:63,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.h5,{children:\"Conclusion\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:65,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[\"OpenAI\\u2019s \",(0,a.jsxDEV)(n.strong,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:67,columnNumber:10},this),\" is poised to revolutionize how we think about video creation, making it easier for creators to bring their ideas to life. With its ability to produce realistic, dynamic videos from text, \",(0,a.jsxDEV)(n.em,{children:\"Sora\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:67,columnNumber:206},this),\" has the potential to reshape industries ranging from filmmaking to digital marketing. Stay tuned as this groundbreaking \",(0,a.jsxDEV)(n.strong,{children:\"AI\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:67,columnNumber:333},this),\" technology rolls out publicly later in 2024.\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:67,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[\"For updates and further details, follow OpenAI\\u2019s official channels and stay ahead of the latest in \",(0,a.jsxDEV)(n.em,{children:\"AI\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:69,columnNumber:100},this),\" innovations.\"]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:69,columnNumber:1},this),`\n`,(0,a.jsxDEV)(n.p,{children:[(0,a.jsxDEV)(n.strong,{children:\"#ALLHAILAI\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:71,columnNumber:1},this),(0,a.jsxDEV)(n.br,{},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:71,columnNumber:15},this),`\n`,(0,a.jsxDEV)(n.em,{children:\"#PromptArtist\"},void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:72,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:71,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\",lineNumber:1,columnNumber:1},this)}function gn(f={}){let{wrapper:n}=f.components||{};return n?(0,a.jsxDEV)(n,Object.assign({},f,{children:(0,a.jsxDEV)(Ae,f,void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\"},this)}),void 0,!1,{fileName:\"/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx\"},this):Ae(f)}var xn=gn;function En(f,n,m){throw new Error(\"Expected \"+(n?\"component\":\"object\")+\" `\"+f+\"` to be defined: you likely forgot to import, pass, or provide it.\"+(m?\"\\nIt\\u2019s referenced in your code at `\"+m+\"` in `/Users/nota/na-now-news/posts/_mdx_bundler_entry_point-211bc1ba-f6d9-4ea5-af88-663b2f4a4f8c.mdx`\":\"\"))}return vn(Sn);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "latest-leap-in-ai-video-generation.mdx",
  "_raw": {
    "sourceFilePath": "latest-leap-in-ai-video-generation.mdx",
    "sourceFileName": "latest-leap-in-ai-video-generation.mdx",
    "sourceFileDir": ".",
    "contentType": "mdx",
    "flattenedPath": "latest-leap-in-ai-video-generation"
  },
  "type": "Post",
  "url": "/posts/latest-leap-in-ai-video-generation"
}